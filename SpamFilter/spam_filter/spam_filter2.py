import os
import sys
import subprocess
from pyspark.sql import SparkSession

# 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π –∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è
# --------------------------------------------------
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ä–µ–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ Java 17 (–Ω–∞–π–¥–∏—Ç–µ —Å –ø–æ–º–æ—â—å—é: update-alternatives --list java)
JAVA_HOME_PATH = "/usr/lib/jvm/java-17-openjdk-amd64"

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
os.environ["JAVA_HOME"] = JAVA_HOME_PATH
os.environ["PATH"] = f"{JAVA_HOME_PATH}/bin:{os.environ['PATH']}"
os.environ["PYSPARK_PYTHON"] = sys.executable
os.environ["PYSPARK_DRIVER_PYTHON"] = sys.executable

# 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
# --------------------------------------------------
print("="*50)
print("Environment Diagnostics")
print("="*50)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Java
try:
    java_version = subprocess.check_output(
        ["java", "-version"],
        stderr=subprocess.STDOUT,
        universal_newlines=True
    )
    print(f"‚úì Java version:\n{java_version}")
except Exception as e:
    print(f"‚úó Java check failed: {e}")
    sys.exit(1)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Python
print(f"‚úì Python executable: {sys.executable}")
print(f"‚úì Python version: {sys.version}")

# 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
# --------------------------------------------------
print("\n" + "="*50)
print("Initializing Spark Session")
print("="*50)

try:
    spark = SparkSession.builder \
        .appName("SpamFilter") \
        .config("spark.driver.memory", "1g") \
        .config("spark.executor.memory", "1g") \
        .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
        .config("spark.driver.extraJavaOptions", 
                "--add-opens=java.base/java.lang=ALL-UNNAMED "
                "--add-opens=java.base/java.nio=ALL-UNNAMED "
                "--add-opens=java.base/java.util=ALL-UNNAMED "
                "-XX:+IgnoreUnrecognizedVMOptions") \
        .config("spark.executor.extraJavaOptions", 
                "--add-opens=java.base/java.lang=ALL-UNNAMED "
                "--add-opens=java.base/java.nio=ALL-UNNAMED "
                "--add-opens=java.base/java.util=ALL-UNNAMED "
                "-XX:+IgnoreUnrecognizedVMOptions") \
        .getOrCreate()
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ Spark
    print(f"‚úì Spark version: {spark.version}")
    sc = spark.sparkContext
    print(f"‚úì Java version in Spark: {sc._jvm.java.lang.System.getProperty('java.version')}")
    
    # –¢–µ—Å—Ç–æ–≤–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è
    data = [("Test", 1), ("Spark", 2)]
    df = spark.createDataFrame(data, ["Word", "Count"])
    print("‚úì Simple DataFrame test:")
    print(df.show())
    
    print("\n‚úÖ Spark initialized successfully!")

except Exception as e:
    print(f"\n‚ùå Spark initialization failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# 5. –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
# --------------------------------------------------
try:
    print("\n" + "="*50)
    print("Running Spam Filter")
    print("="*50)
    
    # –ó–¥–µ—Å—å –≤–∞—à –∫–æ–¥ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—å—é
    # –ù–∞–ø—Ä–∏–º–µ—Ä:
    # model = PipelineModel.load("path/to/model")
    # results = model.transform(your_data)
    # results.show()
    
    print("‚úÖ Spam filter operations completed!")

except Exception as e:
    print(f"\n‚ùå Error in main application: {e}")
    import traceback
    traceback.print_exc()
    
finally:
    # –í—Å–µ–≥–¥–∞ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Spark
    spark.stop()
    print("\nüõë Spark session stopped")